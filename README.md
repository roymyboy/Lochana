# Lochana â€“ Real-Time Vision Copilot

> Updated November 2025 â€“ reflects the current development branch with YOLOv11 object detection, conversational UI, OCR mode, and GPT-4 Omni integrations.

## ğŸ¯ Overview

Lochana turns an Android device into a multimodal assistant that combines:

- **On-device vision** powered by a YOLOv11 nano ONNX model for continuous object detection overlays.
- **Conversational AI** with GPT-4 Omni for rich scene descriptions, custom prompts, and video summaries.
- **Accessibility tooling** including live OCR, text-to-speech playback, adjustable chat typography, and haptic feedback.
- **Camera-first UX** featuring gesture controls, smart capture flows, and snapshots embedded directly inside the chat timeline.

The result is a responsive copilot that can describe what it sees, answer follow-up questions, read text it finds in the world, and keep users in control of every request.

---

## âœ¨ Highlights

**On-Device Vision**
- YOLOv11n (ONNX Runtime Mobile) draws real-time detections with class labels, confidences, and temporal smoothing via `DetectionTracker`.
- `DetectionOverlayView` matches the `PreviewView` crop and renders anti-aliased bounding boxes at 20â€¯fps+ on modern hardware.
- Camera-side optimisations (auto focus gating, zoom smoothing, extension selection) live in `CameraManager`.

**Conversational Analysis**
- Manual capture flow: tap the capture button to send a single stabilized frame; long-press to collect a 5-second burst (up to 5 frames) for aggregated GPT-4 Omni video analysis.
- `SnapshotManager` stores downsized thumbnails so responses include tappable image previews inside the chat.
- Chat bubbles support typewriter playback, copy-to-clipboard, per-message TTS, and animated status indicators.

**Dual Capture Modes**
- `Analysis` mode pipes still frames or multi-frame clips to GPT-4 Omni with project-specific instructions.
- `OCR` mode invokes ML Kit Text Recognition (Latin) for high-confidence transcription of signage, documents, or packaging.
- Mode switching is instant from a Material exposed dropdown in the capture toolbar.

**Voice & Accessibility**
- `SpeechController` offers push-to-talk prompting (speech-to-text) and natural text-to-speech playback of assistant messages.
- Adjustable chat font scaling, persistent typing indicators, haptic confirmations, and animated focus reticules improve usability in the field.
- Status keywords keep operational updates ("Analyzingâ€¦", "Capturingâ€¦") distinct from assistant answers.

**Resilient Interaction Loop**
- `DetectionManager` isolates OpenAI capture from YOLO inference and protects active responses from being overwritten until a meaningful scene change occurs.
- `CrashHandler` surfaces fatal errors with user-friendly toasts and logging breadcrumbs.
- Rich diagnostics (camera readiness, preview sizing, throughput) simplify debugging on real hardware.

---

## ğŸ— Architecture at a Glance

```
MainActivity
â”œâ”€ CameraManager              â†’ CameraX lifecycle, focus, zoom, device extensions
â”œâ”€ DetectionManager           â†’ Manual capture flow, scene differencing, YOLO smoothing
â”‚  â””â”€ YOLOv11Manager          â†’ ONNX Runtime session + post processing
â”œâ”€ OpenAIManager              â†’ GPT-4 Omni chat completions (single-frame & multi-frame)
â”‚  â””â”€ ConfigLoader            â†’ Loads API key from assets or shared prefs
â”‚  â””â”€ OpenAIKeyManager        â†’ Lightweight persisted storage for runtime keys
â”œâ”€ OcrProcessor               â†’ ML Kit Text Recognition (Latin)
â”œâ”€ PermissionManager          â†’ Camera, microphone, and storage permission UX
â””â”€ UIManager                  â†’ Gesture handling, capture UX, status HUD
   â”œâ”€ ChatController          â†’ Chat timeline, snapshots, copy/TTS actions
   â”œâ”€ PromptController        â†’ Prompt entry, keyboard orchestration, font scaling
   â”œâ”€ SpeechController        â†’ Speech recognition + text-to-speech pipeline
   â”œâ”€ SnapshotManager         â†’ Ephemeral JPEG cache for chat previews
   â””â”€ PreviewDialogController â†’ Full-screen preview of captured imagery
```

Key asset pipeline:

- `app/src/main/assets/yolov11n.onnx` â€“ 640â€¯px YOLOv11 nano model.
- `app/src/main/assets/openai_instructions.json` â€“ system & user prompt templates for single frame vs. video analysis.
- `app/src/main/assets/config.properties` â€“ optional (gitignored) OpenAI API key container.

---

## ğŸ“‹ Requirements

- **Android**: API 24 (Nougat) or higher. 64-bit devices recommended for ONNX performance.
- **Permissions**: Camera, internet, microphone (for voice prompts & TTS).
- **Connectivity**: Stable network for OpenAI calls; OCR and YOLO run locally.
- **Memory**: 2â€¯GB RAM minimum; 4â€¯GB+ recommended for smooth multi-frame captures.
- **Tooling**: Android Studio (latest stable), Gradle Wrapper bundled with repo.

---

## âš™ï¸ Setup & Configuration

### 1. Clone & Open
```bash
git clone <repository-url>
cd Lochana
```
Open the project in Android Studio and let Gradle sync.

### 2. Configure the OpenAI API Key

Use one of the options described in `docs/CONFIG_SETUP.md`. The typical workflow:

1. Copy the template into place (the destination file is ignored by git):
   ```bash
   cp app/src/main/assets/config.properties.template app/src/main/assets/config.properties
   ```
2. Add your key:
   ```
   OPENAI_API_KEY=sk-your-api-key
   ```
3. (Optional) At runtime you can also inject a key via `OpenAIKeyManager` (stored obfuscated in `SharedPreferences`).

### 3. Build & Install

Quick script (Windows PowerShell):
```powershell
.\quick-install.bat
```

Manual Gradle flow:
```bash
.\gradlew assembleDebug
adb install -r app/build/outputs/apk/debug/app-debug.apk
```

---

## ğŸš€ Using Lochana

1. **Launch & Permit**  
   Grant camera (and microphone if you plan to use voice input) permissions when prompted.

2. **Live Vision Feed**  
   The YOLO overlay appears immediately; detections persist with temporal smoothing. Use pinch to zoom, single tap to focus (with animated reticule), and double tap to swap cameras.

3. **Compose Prompts (Optional)**  
   Type into the prompt field or tap the microphone icon to dictate a question. The prompt area supports multiline text and shift+enter for new lines.

4. **Capture for GPT Analysis**  
   - *Single tap* the capture button (blue ring) to grab a stabilized still. The app buffers frames until conditions are steady, then submits one snapshot to GPT-4 Omni together with your prompt and displays a thumbnail in the chat.
   - *Long press* (Analysis mode only) to collect a 5-second burst. Up to five frames are aggregated into a single GPT video request for richer motion awareness. A progress animation plays while frames buffer.

5. **Switch Modes**  
   Open the mode dropdown beside the capture button:
   - `Scene Analysis` â†’ GPT-4 Omni (vision) responses.
   - `Text OCR` â†’ ML Kit text recognition. Results appear instantly in the chat; long-press capture is disabled and the UI warns if attempted.

6. **Review & Interact**  
   - Assistant replies stream in with a typewriter effect and status indicator.  
   - Tap image previews to open a full-resolution dialog.  
   - Copy responses or trigger text-to-speech per message using the inline action row.  
   - Adjust chat font size via pinch gesture inside the chat pane.

7. **Voice Output & Input**  
   The microphone toggles speech recognition; the speaker icon on each message hands off text-to-speech with proper lifecycle cleanup when playback ends or errors.

8. **Resetting & Stability**  
   The detection pipeline protects completed responses until meaningful scene movement (>10â€¯% pixel delta) is observed. If you need to force a new capture, tap the capture button again or double tap to switch cameras (which flushes history).

---

## ğŸ”¬ Implementation Notes

- **Scene Differencing**: `DetectionManager` performs YUV â†’ NV21 â†’ JPEG conversion, rotates frames, and compares pixel deltas with a 10â€¯% movement threshold before allowing new GPT captures.
- **Temporal Tracking**: `DetectionTracker` associates YOLO detections across frames, clearing stale tracks after a single empty frame to avoid ghost boxes.
- **OpenAI Safeguards**: Requests are serialized (`isProcessing` flag). Errors (timeouts, 401/429, SSL) bubble up with contextual toasts and chat status messages. Video captures obey a 60â€¯s HTTP timeout and support up to 5 frames.
- **Snapshot Lifecycle**: Previews are sized to â‰¤720px, compressed at 80â€¯% JPEG, stored in the appâ€™s cache (`chat_snapshots`). The cache trims older entries to the most recent 30.
- **Voice Pipeline**: Speech recognition checks microphone permission at runtime, provides partial results, and animates the mic button while active. TTS uses `AudioManager.STREAM_MUSIC` and cleans up utterances when playback stops.
- **Error Handling**: `CrashHandler` intercepts uncaught exceptions to display user guidance before terminating, improving resilience during field testing.

---

## ğŸ“‚ Project Structure (Condensed)

```
app/
â”œâ”€â”€ src/main/java/com/lochana/app/
â”‚   â”œâ”€â”€ MainActivity.kt
â”‚   â”œâ”€â”€ CameraManager.kt
â”‚   â”œâ”€â”€ DetectionManager.kt
â”‚   â”œâ”€â”€ DetectionOverlayView.kt
â”‚   â”œâ”€â”€ DetectionTracker.kt
â”‚   â”œâ”€â”€ YOLOv11Manager.kt
â”‚   â”œâ”€â”€ OpenAIManager.kt
â”‚   â”œâ”€â”€ OpenAIKeyManager.kt
â”‚   â”œâ”€â”€ ConfigLoader.kt
â”‚   â”œâ”€â”€ UIManager.kt
â”‚   â”œâ”€â”€ PermissionManager.kt
â”‚   â”œâ”€â”€ CrashHandler.kt
â”‚   â”œâ”€â”€ ui/
â”‚   â”‚   â”œâ”€â”€ ChatController.kt
â”‚   â”‚   â”œâ”€â”€ PromptController.kt
â”‚   â”‚   â”œâ”€â”€ SpeechController.kt
â”‚   â”‚   â”œâ”€â”€ PreviewDialogController.kt
â”‚   â”‚   â”œâ”€â”€ SnapshotManager.kt
â”‚   â””â”€â”€ ocr/
â”‚       â””â”€â”€ OcrProcessor.kt
â”œâ”€â”€ src/main/assets/
â”‚   â”œâ”€â”€ config.properties.template
â”‚   â”œâ”€â”€ openai_instructions.json
â”‚   â””â”€â”€ yolov11n.onnx
â””â”€â”€ src/main/res/â€¦ (layouts, drawables, styles)
```

---

## ğŸ›  Troubleshooting

- **Black Preview / No Camera Feed**  
  Confirm no other app is using the camera. The logcat tag `CameraManager` surfaces retry attempts; the UI presents troubleshooting text if binding fails.

- **Missing YOLO Detections**  
  Ensure the ONNX model exists in `app/src/main/assets`. Older or 32-bit devices may struggle with ONNX Runtime 1.18.0â€”monitor `YOLOv11Manager` logs for `session` errors.

- **GPT Requests Failing**  
  Verify `OPENAI_API_KEY` is set, device has internet, and your account has access to GPT-4o. The app reports HTTP status-derived guidance in the chat.

- **OCR Mode Returns Empty Text**  
  Good lighting and legible fonts are required. The app emits `toast_ocr_no_text` when ML Kit returns an empty payload.

- **Speech Features Disabled**  
  You must grant microphone permission. Check `SpeechController` logs for permission or recognizer failures.

- **Out of Memory During Capture**  
  The app aggressively recycles bitmaps, but older devices may need smaller capture resolutions. Watch for `OutOfMemoryError` in `DetectionManager` logs and reduce usage of prolonged long-press captures.

---

## ğŸš§ Roadmap Ideas

- Configurable YOLO class lists and thresholds.
- Offline small-language-model fallback for summary generation.
- Persistent analysis history with export and share actions.
- Multi-language OCR/TTS pipelines and translation overlays.
- Guided capture workflows (e.g., "scan document" vs "describe scene" presets).

---

## ğŸ“„ License & Attribution

Lochana is distributed for educational and demonstration purposes showcasing modern Android multimodal design. External assets include:

- YOLOv11 ONNX model (COCO classes) â€“ see original license terms.
- OpenAI GPT-4 Omni API â€“ subject to OpenAI usage policies.
- Google ML Kit Text Recognition â€“ governed by Google Play Services terms.

---

**Built with â¤ï¸ using Kotlin, CameraX, ONNX Runtime, ML Kit, and GPT-4 Omni.**
